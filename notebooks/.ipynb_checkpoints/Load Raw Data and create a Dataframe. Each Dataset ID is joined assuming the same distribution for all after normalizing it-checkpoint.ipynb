{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T16:26:32.653613Z",
     "start_time": "2018-01-10T16:26:29.997788Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using TensorFlow backend.\n",
      "/home/rneves/anaconda3/envs/python3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "from helper import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T16:29:26.824426Z",
     "start_time": "2018-01-10T16:29:26.055156Z"
    },
    "code_folding": [
     75,
     126,
     187,
     201,
     254
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_RUL_label():\n",
    "    \n",
    "    print('Creating RUL label....')\n",
    "    \n",
    "    RUL_train = df_train.groupby(['dataset_id', 'unit_id'])['cycle'].max()\n",
    "    RUL_test = df_test.groupby(['dataset_id', 'unit_id'])['cycle'].max() + df_RUL.groupby(['dataset_id', 'unit_id'])['rul'].max()\n",
    "\n",
    "    df_train['RUL'] = df_train.apply(lambda r: get_RUL(r, RUL_train), axis=1)\n",
    "    df_test['RUL'] = df_test.apply(lambda r: get_RUL(r, RUL_test), axis=1)\n",
    "    \n",
    "    print('Done!!')\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def split_dataset_by_type():\n",
    "    \n",
    "    print('Splitting datasets...')\n",
    "    \n",
    "    type_1 = ['FD001', 'FD003']\n",
    "    type_2 = ['FD002', 'FD004']\n",
    "    \n",
    "    df_train_type1 = df_train[df_train['dataset_id'].isin(type_1)].reset_index(drop=True)\n",
    "    df_train_type2 = df_train[df_train['dataset_id'].isin(type_2)].reset_index(drop=True)\n",
    "\n",
    "    df_test_type1 = df_test[df_test['dataset_id'].isin(type_1)].reset_index(drop=True)\n",
    "    df_test_type2 = df_test[df_test['dataset_id'].isin(type_2)].reset_index(drop=True)\n",
    "\n",
    "    df_RUL_type1 = df_RUL[df_RUL['dataset_id'].isin(type_1)].reset_index(drop=True)\n",
    "    df_RUL_type2 = df_RUL[df_RUL['dataset_id'].isin(type_2)].reset_index(drop=True)\n",
    "    \n",
    "    print('Done!!')\n",
    "    \n",
    "    return df_train_type1, df_train_type2, df_test_type1, df_test_type2, df_RUL_type1, df_RUL_type2\n",
    "\n",
    "def normalize_dataset():\n",
    "    \n",
    "    print('Normalizing datasets...')\n",
    "    \n",
    "    df_train_type1_normalize = df_train_type1.copy()\n",
    "    df_test_type1_normalize = df_test_type1.copy()\n",
    "\n",
    "    scaler_type1 = StandardScaler().fit(df_train_type1[sensor_columns])\n",
    "    df_train_type1_normalize[sensor_columns] = scaler_type1.transform(df_train_type1[sensor_columns])\n",
    "    df_test_type1_normalize[sensor_columns] = scaler_type1.transform(df_test_type1[sensor_columns])\n",
    "\n",
    "\n",
    "    df_train_type2_normalize = df_train_type2.copy()\n",
    "    df_test_type2_normalize = df_test_type2.copy()\n",
    "\n",
    "    gb = df_train_type2.groupby('HDBScan')[sensor_columns]\n",
    "        \n",
    "    d={}\n",
    "\n",
    "    for x in gb.groups:\n",
    "        d[\"scaler_type2_{0}\".format(x)] = StandardScaler().fit(gb.get_group(x))\n",
    "        df_train_type2_normalize.loc[df_train_type2_normalize['HDBScan'] == x, sensor_columns] = d[\"scaler_type2_{0}\".format(x)].transform(df_train_type2.loc[df_train_type2['HDBScan'] == x, sensor_columns]) \n",
    "        df_test_type2_normalize.loc[df_test_type2_normalize['HDBScan'] == x, sensor_columns] = d[\"scaler_type2_{0}\".format(x)].transform(df_test_type2.loc[df_test_type2['HDBScan'] == x, sensor_columns]) \n",
    "    \n",
    "    print('Done!!')\n",
    "    \n",
    "    return df_train_type1_normalize, df_train_type2_normalize, df_test_type1_normalize, df_test_type2_normalize\n",
    "\n",
    "def clustering_dataset():\n",
    "    \n",
    "    print('Clustering...')\n",
    "    \n",
    "    columns = ['']\n",
    "\n",
    "    HDBScan_clustering(df_train_type1, df_test_type1, columns = ['setting 1', 'setting 2', 'setting 3'])\n",
    "    HDBScan_clustering(df_train_type2, df_test_type2, columns = ['setting 1', 'setting 2', 'setting 3'])\n",
    "    \n",
    "    print('Done!!')\n",
    "    \n",
    "    return df_train_type1, df_train_type2, df_test_type1, df_test_type2\n",
    "\n",
    "def join_datasets():\n",
    "    \n",
    "    print('Joining datasets...')\n",
    "    \n",
    "    df_train_all = pd.concat([df_train_type1, df_train_type2]).reset_index(drop=True)\n",
    "    df_test_all = pd.concat([df_test_type1, df_test_type2]).reset_index(drop=True)\n",
    "\n",
    "    df_train_all_normalize = pd.concat([df_train_type1_normalize, df_train_type2_normalize]).reset_index(drop=True)\n",
    "    df_test_all_normalize = pd.concat([df_test_type1_normalize, df_test_type2_normalize]).reset_index(drop=True)\n",
    "    \n",
    "    print('Done!!')\n",
    "    \n",
    "    return df_train_all, df_test_all, df_train_all_normalize, df_test_all_normalize\n",
    "\n",
    "def differentiate_sensors():\n",
    "    \n",
    "    print('Differentiating sensors readings...')\n",
    "    \n",
    "    df_train_differentiated = df_train_all_normalize.copy()\n",
    "    df_test_differentiated = df_test_all_normalize.copy()\n",
    "\n",
    "    unit = ['FD001', 'FD002', 'FD003', 'FD004']\n",
    "\n",
    "    for unit_id in unit:\n",
    "        units = df_train_all_normalize[df_train_all_normalize['dataset_id'] == unit_id]['unit_id'].unique()\n",
    "        #print('Differentiating dataset {} from training set'.format(unit_id))\n",
    "\n",
    "        for unit_number in units:\n",
    "            for i in range (0,21):\n",
    "\n",
    "                df_train_differentiated.loc[(df_train_differentiated.dataset_id == unit_id) & \\\n",
    "                                            (df_train_differentiated.unit_id == unit_number), 'sensor '+ '{}'.format(i+1)] \\\n",
    "                = timeseries_difference(df_train_all_normalize.loc[(df_train_all_normalize.dataset_id == unit_id) & \\\n",
    "                                        (df_train_all_normalize.unit_id == unit_number)][sensor_columns].iloc[:,i], diff_lag = 10).copy()                \n",
    "\n",
    "    for unit_id in unit:\n",
    "        units = df_test_all_normalize[df_test_all_normalize['dataset_id'] == unit_id]['unit_id'].unique()\n",
    "        #print('Differentiating dataset {} from test set'.format(unit_id))\n",
    "        \n",
    "        for unit_number in units:\n",
    "            for i in range (0,21):\n",
    "\n",
    "                df_test_differentiated.loc[(df_test_differentiated.dataset_id == unit_id) & \\\n",
    "                                            (df_test_differentiated.unit_id == unit_number), 'sensor '+ '{}'.format(i+1)] \\\n",
    "                = timeseries_difference(df_test_all_normalize.loc[(df_test_all_normalize.dataset_id == unit_id) & \\\n",
    "                                        (df_test_all_normalize.unit_id == unit_number)][sensor_columns].iloc[:,i], diff_lag = 10).copy()\n",
    "    \n",
    "    print('Done!!')\n",
    "    \n",
    "    return df_train_differentiated, df_test_differentiated\n",
    "\n",
    "def get_rolling_features():\n",
    "    \n",
    "    print('Calculating rolling features...')\n",
    "    \n",
    "    dfs = ['FD001','FD002', 'FD003', 'FD004']\n",
    "\n",
    "    rolling_feature = [{\"min\": min}, {\"max\": max}, {\"mean\": np.mean}, {\"std\": np.std}, {\"median\": np.median}]\n",
    "    rolling_size = [5, 10, 20]\n",
    "\n",
    "    df_train_final = df_train_all_normalize.copy()\n",
    "    df_train_final.reset_index(inplace=True)\n",
    "\n",
    "    for i ,x in enumerate(rolling_feature):\n",
    "        for size in rolling_size:\n",
    "\n",
    "            df = []\n",
    "            df_rolling=[]\n",
    "\n",
    "            for dataset_id in dfs:\n",
    "\n",
    "                df_train_final.groupby(['dataset_id', 'unit_id'])\n",
    "                all_units = df_train_final[df_train_final['dataset_id'] == dataset_id]['unit_id'].unique()\n",
    "                plot_data = df_train_final[(df_train_final['dataset_id'] == dataset_id) & (df_train_final['unit_id'].isin(all_units))].copy()\n",
    "\n",
    "                for unit_id, group in plot_data.groupby('unit_id'):\n",
    "\n",
    "                    df.append(group[sensor_columns].rolling(size).aggregate(x).fillna(method='bfill'))\n",
    "\n",
    "            df_rolling = pd.concat(df, ignore_index=True)\n",
    "            df_rolling.columns = sensor_columns\n",
    "\n",
    "            df_train_final = df_train_final.join(df_rolling, rsuffix='_rolling_{0}_{1}'.format(str([*rolling_feature[i]][0]), str(size))).copy()\n",
    "\n",
    "    df_test_final = df_test_all_normalize.copy()\n",
    "    df_test_final.reset_index(inplace=True)\n",
    "\n",
    "    for i ,x in enumerate(rolling_feature):\n",
    "        for size in rolling_size:\n",
    "\n",
    "            df = []\n",
    "            df_rolling=[]\n",
    "\n",
    "            for dataset_id in dfs:\n",
    "\n",
    "                df_test_final.groupby(['dataset_id', 'unit_id'])\n",
    "                all_units = df_test_final[df_test_final['dataset_id'] == dataset_id]['unit_id'].unique()\n",
    "                plot_data = df_test_final[(df_test_final['dataset_id'] == dataset_id) & (df_test_final['unit_id'].isin(all_units))].copy()\n",
    "\n",
    "                for unit_id, group in plot_data.groupby('unit_id'):\n",
    "\n",
    "                    df.append(group[sensor_columns].rolling(size).aggregate(x).fillna(method='bfill'))\n",
    "\n",
    "            df_rolling = pd.concat(df, ignore_index=True)\n",
    "            df_rolling.columns = sensor_columns\n",
    "\n",
    "            df_test_final = df_test_final.join(df_rolling, rsuffix='_rolling_{0}_{1}'.format(str([*rolling_feature[i]][0]), str(size))).copy()\n",
    "\n",
    "    print('Done!!')\n",
    "    \n",
    "    return df_train_final, df_test_final\n",
    "\n",
    "def get_rolling_features_diff():\n",
    "    \n",
    "    print('Calculating rolling features for differentiated sensors readings...')\n",
    "    \n",
    "    dfs = ['FD001','FD002', 'FD003', 'FD004']\n",
    "    settings = ['setting 1', 'setting 2', 'setting 3','dataset_id', \n",
    "                'RUL', 'cycle', 'unit_id', 'HDBScan']\n",
    "\n",
    "    rolling_feature = [{\"min\": min}, {\"max\": max}, {\"mean\": np.mean}, {\"std\": np.std}, {\"median\": np.median}]\n",
    "    rolling_size = [5, 10, 20]\n",
    "    \n",
    "    df_train_final_diff = df_train_differentiated.copy()\n",
    "    df_train_final_diff.reset_index(inplace=True)\n",
    "\n",
    "    for i ,x in enumerate(rolling_feature):\n",
    "        for size in rolling_size:\n",
    "\n",
    "            df = []\n",
    "            df_rolling=[]\n",
    "\n",
    "            for dataset_id in dfs:\n",
    "\n",
    "                df_train_final_diff.groupby(['dataset_id', 'unit_id'])\n",
    "                all_units = df_train_final_diff[df_train_final_diff['dataset_id'] == dataset_id]['unit_id'].unique()\n",
    "                plot_data = df_train_final_diff[(df_train_final_diff['dataset_id'] == dataset_id) & (df_train_final_diff['unit_id'].isin(all_units))].copy()\n",
    "\n",
    "                for unit_id, group in plot_data.groupby('unit_id'):\n",
    "\n",
    "                    df.append(group[sensor_columns].rolling(size).aggregate(x).fillna(method='bfill'))\n",
    "\n",
    "            df_rolling = pd.concat(df, ignore_index=True)\n",
    "            df_rolling.columns = sensor_columns\n",
    "\n",
    "            df_train_final_diff = df_train_final_diff.join(df_rolling, rsuffix='_rolling_{0}_{1}'.format(str([*rolling_feature[i]][0]), str(size))).copy()\n",
    "\n",
    "    df_train_final_diff.drop((settings), axis=1, inplace=True)\n",
    "    \n",
    "    df_test_final_diff = df_test_differentiated.copy()\n",
    "    df_test_final_diff.reset_index(inplace=True)\n",
    "\n",
    "    for i ,x in enumerate(rolling_feature):\n",
    "        for size in rolling_size:\n",
    "\n",
    "            df = []\n",
    "            df_rolling=[]\n",
    "\n",
    "            for dataset_id in dfs:\n",
    "\n",
    "                df_test_final_diff.groupby(['dataset_id', 'unit_id'])\n",
    "                all_units = df_test_final_diff[df_test_final_diff['dataset_id'] == dataset_id]['unit_id'].unique()\n",
    "                plot_data = df_test_final_diff[(df_test_final_diff['dataset_id'] == dataset_id) & (df_test_final_diff['unit_id'].isin(all_units))].copy()\n",
    "\n",
    "                for unit_id, group in plot_data.groupby('unit_id'):\n",
    "\n",
    "                    df.append(group[sensor_columns].rolling(size).aggregate(x).fillna(method='bfill'))\n",
    "\n",
    "            df_rolling = pd.concat(df, ignore_index=True)\n",
    "            df_rolling.columns = sensor_columns\n",
    "\n",
    "            df_test_final_diff = df_test_final_diff.join(df_rolling, rsuffix='_rolling_{0}_{1}'.format(str([*rolling_feature[i]][0]), str(size))).copy()\n",
    "    \n",
    "    df_test_final_diff.drop((settings), axis=1, inplace=True)\n",
    "    \n",
    "    return df_train_final_diff, df_test_final_diff\n",
    "    \n",
    "    print('Done!!')\n",
    "\n",
    "def join_final_datasets_and_save_to_df():\n",
    "    \n",
    "    print('Joining final datasets and saving to dataframes...')\n",
    "    \n",
    "    train_final = df_train_final.join(df_train_final_diff, rsuffix='_diff').copy()\n",
    "    test_final = df_test_final.join(df_test_final_diff, rsuffix='_diff').copy()\n",
    "\n",
    "    train_final.to_pickle(path + 'dataframes/train__1_diff_10')\n",
    "    test_final.to_pickle(path + 'dataframes/test__1_diff_10')\n",
    "    \n",
    "    print('Done!!')\n",
    "    \n",
    "    return train_final, test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-10T16:29:30.657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_FD003.txt\n",
      "train_FD002.txt\n",
      "test_FD004.txt\n",
      "RUL_FD003.txt\n",
      "test_FD001.txt\n",
      "train_FD001.txt\n",
      "RUL_FD001.txt\n",
      "RUL_FD002.txt\n",
      "train_FD003.txt\n",
      "train_FD004.txt\n",
      "test_FD002.txt\n",
      "RUL_FD004.txt\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "data = '/datadrive/Turbofan_Engine/' #path to .txt files\n",
    "path = '/home/rneves/thesis/Turbofan_Engine/' # path to numpy arrays to plot t-sne\n",
    "\n",
    "text_files = [f for f in os.listdir(data) if f.endswith('.txt') and not f.startswith('r')]\n",
    "dataframe = [os.path.splitext(f)[0] for f in text_files]\n",
    "\n",
    "sensor_columns = [\"sensor {}\".format(s) for s in range(1,22)]\n",
    "sensor_columns_rolling_mean = [\"sensor {}_rolling_mean\".format(s) for s in range(1,22)]\n",
    "sensor_columns_rolling_std = [\"sensor {}_rolling_std\".format(s) for s in range(1,22)]\n",
    "\n",
    "info_columns = ['dataset_id', 'unit_id','cycle','setting 1', 'setting 2', 'setting 3']\n",
    "\n",
    "label_columns = ['dataset_id', 'unit_id', 'rul']\n",
    "\n",
    "settings = ['setting 1', 'setting 2', 'setting 3']\n",
    "\n",
    "test_data = []\n",
    "train_data = []\n",
    "RUL_data = []\n",
    "\n",
    "for file in text_files:\n",
    "    print(file)\n",
    "    \n",
    "    if re.match('RUL*', file):\n",
    "        subset_df = pd.read_csv(data+file, delimiter=r\"\\s+\", header=None)\n",
    "        unit_id = range(1, subset_df.shape[0] + 1)\n",
    "        subset_df.insert(0, 'unit_id', unit_id)\n",
    "        dataset_id = basename(file).split(\"_\")[1][:5]\n",
    "        subset_df.insert(0, 'dataset_id', dataset_id)    \n",
    "        RUL_data.append(subset_df)\n",
    "    \n",
    "    if re.match('test*', file):\n",
    "        subset_df = pd.read_csv(data+file, delimiter=r\"\\s+\", header=None, usecols=range(26))\n",
    "        dataset_id = basename(file).split(\"_\")[1][:5]\n",
    "        subset_df.insert(0, 'dataset_id', dataset_id)  \n",
    "        test_data.append(subset_df)\n",
    "    \n",
    "    if re.match('train*', file):\n",
    "        subset_df = pd.read_csv(data+file, delimiter=r\"\\s+\", header=None, usecols=range(26))  \n",
    "        dataset_id = basename(file).split(\"_\")[1][:5]\n",
    "        subset_df.insert(0, 'dataset_id', dataset_id)   \n",
    "        train_data.append(subset_df)\n",
    "\n",
    "\n",
    "df_train = pd.concat(train_data, ignore_index=True)\n",
    "df_train.columns = info_columns + sensor_columns               \n",
    "df_train.sort_values(by=['dataset_id', 'unit_id', 'cycle'], inplace=True)\n",
    "\n",
    "df_test = pd.concat(test_data, ignore_index=True)\n",
    "df_test.columns = info_columns + sensor_columns\n",
    "df_test.sort_values(by=['dataset_id', 'unit_id', 'cycle'], inplace=True)\n",
    "\n",
    "df_RUL = pd.concat(RUL_data, ignore_index=True)\n",
    "df_RUL.columns = label_columns\n",
    "df_RUL.sort_values(by=['dataset_id', 'unit_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-10T16:29:31.213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RUL label....\n",
      "Done!!\n",
      "Splitting datasets...\n",
      "Done!!\n",
      "Clustering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/hdbscan/prediction.py:383: UserWarning: Clusterer does not have any defined clusters, new data will be automatically predicted as noise.\n",
      "  warn('Clusterer does not have any defined clusters, new data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters in training set using HDBScan: 1\n",
      "Number of clusters in test set using HDBScan: 1\n",
      "Number of clusters in training set using HDBScan: 6\n",
      "Number of clusters in test set using HDBScan: 6\n",
      "Done!!\n",
      "Normalizing datasets...\n",
      "Done!!\n",
      "Joining datasets...\n",
      "Done!!\n",
      "Differentiating sensors readings...\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = create_RUL_label()\n",
    "df_train_type1, df_train_type2, df_test_type1, df_test_type2, df_RUL_type1, df_RUL_type2 = split_dataset_by_type()\n",
    "df_train_type1, df_train_type2, df_test_type1, df_test_type2 = clustering_dataset()\n",
    "df_train_type1_normalize, df_train_type2_normalize, df_test_type1_normalize, df_test_type2_normalize = normalize_dataset()\n",
    "df_train_all, df_test_all, df_train_all_normalize, df_test_all_normalize = join_datasets()\n",
    "df_train_differentiated, df_test_differentiated = differentiate_sensors()\n",
    "df_train_final, df_test_final = get_rolling_features()\n",
    "df_train_final_diff, df_test_final_diff = get_rolling_features_diff()\n",
    "train_final, test_final = join_final_datasets_and_save_to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
